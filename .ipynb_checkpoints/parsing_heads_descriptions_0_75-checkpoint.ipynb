{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a949269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import idna\n",
    "from fake_useragent import UserAgent\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "\n",
    "# Disable warnings issued by urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44a0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTxt(text):\n",
    "    '''Function is called for cleaning text from trash\n",
    "    INPUT: dirty string\n",
    "    OUTPUT: More or less clean string'''\n",
    "    \n",
    "    \n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    text = re.sub(r'@[А-Яа-я0-9]+', '', text)  # Remove @\n",
    "    text = re.sub(r'#', '', text)  # remove #\n",
    "    text = re.sub(r'{', '', text)\n",
    "    text = re.sub(r'}', '', text)\n",
    "#     text = re.sub(r'.', ' ', text)\n",
    "#     text = re.sub(r',', ' ', text)\n",
    "#     text = re.sub(r'/', '', text)\n",
    "#     text = re.sub(r'\\\\', '', text)\n",
    "    text = re.sub('^а-яА-Я', ' ', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)  # remove hyperlink\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(emoj, '', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    #text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ff20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/mts/unique_values_dict.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "urls = list(urls['url_host'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77c51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "errors = {}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "#     'Referer': 'https://www.google.com/',\n",
    "#     'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "def process_url(url):\n",
    "    try:\n",
    "        url = idna.decode(url)\n",
    "    except:\n",
    "        url = idna.decode(url.encode('ascii'))\n",
    "\n",
    "    if '.turbopages.org' in url:\n",
    "        url = url.replace('.turbopages.org', '').replace('--', '_').replace('-', '.')\n",
    "\n",
    "    try:\n",
    "        response = requests.get('http://' + url, headers=headers, verify=False, timeout=10)      \n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            if response.encoding == 'ISO-8859-1':\n",
    "                \n",
    "                response.encoding = 'UTF-8'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "    #             if soup.find('meta', )\n",
    "                title = soup.title.text.strip()\n",
    "\n",
    "                try:\n",
    "                    header = soup.find('h1').text.strip()\n",
    "                except:\n",
    "                    header = ''\n",
    "                try:\n",
    "                    description = soup.find('meta', attrs={'name': 'description'})['content'].strip()\n",
    "                except:\n",
    "                    description = ''\n",
    "\n",
    "                try:\n",
    "                    keywords = soup.find('meta', attrs={'name': 'keywords'})['content'].strip()\n",
    "                except:\n",
    "                    keywords = '' \n",
    "                    \n",
    "                response.encoding = 'windows-1251'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                title_ = soup.title.text.strip()\n",
    "\n",
    "                try:\n",
    "                    header_ = soup.find('h1').text.strip()\n",
    "                except:\n",
    "                    header_ = ''\n",
    "                try:\n",
    "                    description_ = soup.find('meta', attrs={'name': 'description'})['content'].strip()\n",
    "                except:\n",
    "                    description_ = ''\n",
    "\n",
    "                try:\n",
    "                    keywords_ = soup.find('meta', attrs={'name': 'keywords'})['content'].strip()\n",
    "                except:\n",
    "                    keywords_ = '' \n",
    "                    \n",
    "                data = str(title) + ' ' + str(header) + ' ' + str(description) + ' ' + str(keywords) +\\\n",
    "                            str(title_) + ' ' + str(header_) + ' ' + str(description_) + ' ' + str(keywords)\n",
    "                    \n",
    "            else:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "                title = soup.title.text.strip()\n",
    "\n",
    "                try:\n",
    "                    header = soup.find('h1').text.strip()\n",
    "                except:\n",
    "                    header = ''\n",
    "                try:\n",
    "                    description = soup.find('meta', attrs={'name': 'description'})['content'].strip()\n",
    "                except:\n",
    "                    description = ''\n",
    "\n",
    "                try:\n",
    "                    keywords = soup.find('meta', attrs={'name': 'keywords'})['content'].strip()\n",
    "                except:\n",
    "                    keywords = '' \n",
    "                \n",
    "                data = str(title) + ' ' + str(header) + ' ' + str(description) + ' ' + str(keywords)\n",
    "                \n",
    "            data = cleanTxt(data)\n",
    "            result[url] = data\n",
    "        else:\n",
    "#             print(f\"Error fetching URL {url}. Status code: {response.status_code}\")\n",
    "            result[url] = np.NaN\n",
    "            if response.status_code not in errors:\n",
    "                errors[response.status_code] = []\n",
    "            errors[response.status_code].append(url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "#         print(f'Error fetching URL {url}: {e}')\n",
    "        result[url] = np.NaN\n",
    "#         if e not in  errors:\n",
    "#             errors[e] = []\n",
    "#         errors[e].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65d38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:105: UserWarning: unknown status keyword 'MAIN' in marked section\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    executor.map(process_url, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c2b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(data={'url_host': result.keys(), 'url_info': result.values()})\n",
    "result.to_csv('E:/mts/scrap/urls_info_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ff6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(data={'error': errors.keys(), 'urls': errors.values()})\n",
    "errors.to_csv('E:/mts/scrap/errors_4_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653ed2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('E:/mts/scrap/urls_info_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07262d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_host</th>\n",
       "      <th>url_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last-frontier.ru</td>\n",
       "      <td>главная страница - ильинский рубеж  фильм про ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>app-143608.games.s3.yandex.net</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>обувьлига.рф</td>\n",
       "      <td>обувной центр лига. обувной центр лига обувной...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manasplav.ru</td>\n",
       "      <td>сплав по мане -лето 2023 года! сплав по мане -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>przspb.ru</td>\n",
       "      <td>www.przspb.ru | — наша цель — успех! — наша це...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189158</th>\n",
       "      <td>best-horse.com</td>\n",
       "      <td>best-horse все для всадника и лошади спорта be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189159</th>\n",
       "      <td>sch24.rybadm.ru</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189160</th>\n",
       "      <td>ivanovo.orgsprav.com</td>\n",
       "      <td>справочник города иваново. ивановская область ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189161</th>\n",
       "      <td>biysk.promportal.su</td>\n",
       "      <td>promportal.su - каталог товаров и услуг, интер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189162</th>\n",
       "      <td>onchuka.livejournal.com</td>\n",
       "      <td>onchuka — livejournal onchuka onchuka - новый ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189163 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              url_host  \\\n",
       "0                     last-frontier.ru   \n",
       "1       app-143608.games.s3.yandex.net   \n",
       "2                         обувьлига.рф   \n",
       "3                         manasplav.ru   \n",
       "4                            przspb.ru   \n",
       "...                                ...   \n",
       "189158                  best-horse.com   \n",
       "189159                 sch24.rybadm.ru   \n",
       "189160            ivanovo.orgsprav.com   \n",
       "189161             biysk.promportal.su   \n",
       "189162         onchuka.livejournal.com   \n",
       "\n",
       "                                                 url_info  \n",
       "0       главная страница - ильинский рубеж  фильм про ...  \n",
       "1                                                     NaN  \n",
       "2       обувной центр лига. обувной центр лига обувной...  \n",
       "3       сплав по мане -лето 2023 года! сплав по мане -...  \n",
       "4       www.przspb.ru | — наша цель — успех! — наша це...  \n",
       "...                                                   ...  \n",
       "189158  best-horse все для всадника и лошади спорта be...  \n",
       "189159                                                NaN  \n",
       "189160  справочник города иваново. ивановская область ...  \n",
       "189161  promportal.su - каталог товаров и услуг, интер...  \n",
       "189162  onchuka — livejournal onchuka onchuka - новый ...  \n",
       "\n",
       "[189163 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49883260",
   "metadata": {},
   "source": [
    "# не забыть почистить адреса в главном датасете\n",
    "\n",
    "# в назваиях некоторых сайтов есть имена\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
